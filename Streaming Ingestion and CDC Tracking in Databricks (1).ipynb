{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52c099c-73aa-4576-a22f-98af8660bd67",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCleaning up existing tables and directories...\nCleanup completed successfully\n\nCreating initial Delta table structure...\nInitial table structure created\n\nProcessing Batch 1\n\nGenerating data for batch 1\nWritten batch 1 to /tmp/opportunity_source/batch_1\n\nProcessing batch 1\nExisting IDs in target: []\nCompleted processing batch 1\n\n==================================================\nResults After Batch 1\n==================================================\n\nMain Table Contents:\n+-------------+------+-------+-----------+-----------+-------------------+\n|OpportunityId|  Name| Amount|  StageName|Probability|   LastModifiedDate|\n+-------------+------+-------+-----------+-----------+-------------------+\n|       OPP001|Deal A|10000.0|Prospecting|       20.0|2023-01-01 10:00:00|\n|       OPP002|Deal B|25000.0|Negotiation|       60.0|2023-01-02 11:00:00|\n|       OPP003|Deal C|15000.0|   Proposal|       40.0|2023-01-03 12:00:00|\n+-------------+------+-------+-----------+-----------+-------------------+\n\n\nChanges in Batch 1:\n+-------------+--------------+-------+-----------+-----------+-------------------+\n|OpportunityId|operation_type| Amount|  StageName|Probability|   LastModifiedDate|\n+-------------+--------------+-------+-----------+-----------+-------------------+\n|       OPP001|        INSERT|10000.0|Prospecting|       20.0|2023-01-01 10:00:00|\n|       OPP002|        INSERT|25000.0|Negotiation|       60.0|2023-01-02 11:00:00|\n|       OPP003|        INSERT|15000.0|   Proposal|       40.0|2023-01-03 12:00:00|\n+-------------+--------------+-------+-----------+-----------+-------------------+\n\n\nOperation Summary for Batch 1:\n+--------------+-----+\n|operation_type|count|\n+--------------+-----+\n|        INSERT|    3|\n+--------------+-----+\n\n\nProcessing Batch 2\n\nGenerating data for batch 2\nWritten batch 2 to /tmp/opportunity_source/batch_2\n\nProcessing batch 2\nExisting IDs in target: ['OPP002', 'OPP001', 'OPP003']\nCompleted processing batch 2\n\n==================================================\nResults After Batch 2\n==================================================\n\nMain Table Contents:\n+-------------+------+-------+-------------+-----------+-------------------+\n|OpportunityId|  Name| Amount|    StageName|Probability|   LastModifiedDate|\n+-------------+------+-------+-------------+-----------+-------------------+\n|       OPP001|Deal A|12000.0|Qualification|       30.0|2023-01-04 10:00:00|\n|       OPP002|Deal B|25000.0|  Negotiation|       60.0|2023-01-02 11:00:00|\n|       OPP003|Deal C|15000.0|   Closed Won|      100.0|2023-01-05 12:00:00|\n|       OPP004|Deal D|30000.0|  Prospecting|       20.0|2023-01-05 14:00:00|\n+-------------+------+-------+-------------+-----------+-------------------+\n\n\nChanges in Batch 2:\n+-------------+--------------+-------+-------------+-----------+-------------------+\n|OpportunityId|operation_type| Amount|    StageName|Probability|   LastModifiedDate|\n+-------------+--------------+-------+-------------+-----------+-------------------+\n|       OPP001|        UPDATE|12000.0|Qualification|       30.0|2023-01-04 10:00:00|\n|       OPP003|        UPDATE|15000.0|   Closed Won|      100.0|2023-01-05 12:00:00|\n|       OPP004|        INSERT|30000.0|  Prospecting|       20.0|2023-01-05 14:00:00|\n+-------------+--------------+-------+-------------+-----------+-------------------+\n\n\nOperation Summary for Batch 2:\n+--------------+-----+\n|operation_type|count|\n+--------------+-----+\n|        INSERT|    1|\n|        UPDATE|    2|\n+--------------+-----+\n\n\nProcessing Batch 3\n\nGenerating data for batch 3\nWritten batch 3 to /tmp/opportunity_source/batch_3\n\nProcessing batch 3\nExisting IDs in target: ['OPP001', 'OPP004', 'OPP002', 'OPP003']\nCompleted processing batch 3\n\n==================================================\nResults After Batch 3\n==================================================\n\nMain Table Contents:\n+-------------+------+-------+-----------+-----------+-------------------+\n|OpportunityId|  Name| Amount|  StageName|Probability|   LastModifiedDate|\n+-------------+------+-------+-----------+-----------+-------------------+\n|       OPP001|Deal A|15000.0| Closed Won|      100.0|2023-01-06 10:00:00|\n|       OPP002|Deal B|25000.0|Negotiation|       60.0|2023-01-02 11:00:00|\n|       OPP003|Deal C|15000.0| Closed Won|      100.0|2023-01-05 12:00:00|\n|       OPP004|Deal D|35000.0|Negotiation|       60.0|2023-01-06 14:00:00|\n+-------------+------+-------+-----------+-----------+-------------------+\n\n\nChanges in Batch 3:\n+-------------+--------------+-------+-----------+-----------+-------------------+\n|OpportunityId|operation_type| Amount|  StageName|Probability|   LastModifiedDate|\n+-------------+--------------+-------+-----------+-----------+-------------------+\n|       OPP001|        UPDATE|15000.0| Closed Won|      100.0|2023-01-06 10:00:00|\n|       OPP004|        UPDATE|35000.0|Negotiation|       60.0|2023-01-06 14:00:00|\n+-------------+--------------+-------+-----------+-----------+-------------------+\n\n\nOperation Summary for Batch 3:\n+--------------+-----+\n|operation_type|count|\n+--------------+-----+\n|        UPDATE|    2|\n+--------------+-----+\n\n\n==================================================\nFinal Change Summary\n==================================================\n\nAll Changes by Batch:\n+--------+--------------+-----+\n|batch_id|operation_type|count|\n+--------+--------------+-----+\n|       1|        INSERT|    3|\n|       2|        INSERT|    1|\n|       2|        UPDATE|    2|\n|       3|        UPDATE|    2|\n+--------+--------------+-----+\n\n\nTotal Changes by Operation Type:\n+--------------+-----+\n|operation_type|count|\n+--------------+-----+\n|        INSERT|    4|\n|        UPDATE|    4|\n+--------------+-----+\n\n\nStreaming pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure Delta table settings\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "def get_opportunity_schema():\n",
    "    \"\"\"\n",
    "    Define schema for Opportunity data\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"OpportunityId\", StringType(), False),\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"Amount\", DoubleType(), True),\n",
    "        StructField(\"StageName\", StringType(), True),\n",
    "        StructField(\"CloseDate\", StringType(), True),\n",
    "        StructField(\"LastModifiedDate\", StringType(), True),\n",
    "        StructField(\"AccountId\", StringType(), True),\n",
    "        StructField(\"Probability\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "def cleanup_tables(source_path, target_path, changelog_path):\n",
    "    \"\"\"\n",
    "    Clean up existing tables and directories\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning up existing tables and directories...\")\n",
    "    try:\n",
    "        # Remove directories\n",
    "        dbutils.fs.rm(source_path, recurse=True)\n",
    "        dbutils.fs.rm(target_path, recurse=True)\n",
    "        dbutils.fs.rm(changelog_path, recurse=True)\n",
    "        \n",
    "        # Create source directory\n",
    "        dbutils.fs.mkdirs(source_path)\n",
    "        print(\"Cleanup completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup warning (safe to ignore if first run): {str(e)}\")\n",
    "\n",
    "def generate_sample_data(batch_id=1):\n",
    "    \"\"\"\n",
    "    Generate mock Salesforce Opportunity data for streaming\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating data for batch {batch_id}\")\n",
    "    \n",
    "    if batch_id == 1:\n",
    "        # Initial data\n",
    "        data = [\n",
    "            (\"OPP001\", \"Deal A\", 10000.0, \"Prospecting\", \"2023-01-01\", \"2023-01-01 10:00:00\", \"ACC001\", 20.0),\n",
    "            (\"OPP002\", \"Deal B\", 25000.0, \"Negotiation\", \"2023-02-01\", \"2023-01-02 11:00:00\", \"ACC002\", 60.0),\n",
    "            (\"OPP003\", \"Deal C\", 15000.0, \"Proposal\", \"2023-03-01\", \"2023-01-03 12:00:00\", \"ACC003\", 40.0)\n",
    "        ]\n",
    "    elif batch_id == 2:\n",
    "        # Updates and new record\n",
    "        data = [\n",
    "            (\"OPP001\", \"Deal A\", 12000.0, \"Qualification\", \"2023-01-01\", \"2023-01-04 10:00:00\", \"ACC001\", 30.0),\n",
    "            (\"OPP003\", \"Deal C\", 15000.0, \"Closed Won\", \"2023-03-01\", \"2023-01-05 12:00:00\", \"ACC003\", 100.0),\n",
    "            (\"OPP004\", \"Deal D\", 30000.0, \"Prospecting\", \"2023-04-01\", \"2023-01-05 14:00:00\", \"ACC004\", 20.0)\n",
    "        ]\n",
    "    else:\n",
    "        # Final updates\n",
    "        data = [\n",
    "            (\"OPP001\", \"Deal A\", 15000.0, \"Closed Won\", \"2023-01-01\", \"2023-01-06 10:00:00\", \"ACC001\", 100.0),\n",
    "            (\"OPP004\", \"Deal D\", 35000.0, \"Negotiation\", \"2023-04-01\", \"2023-01-06 14:00:00\", \"ACC004\", 60.0)\n",
    "        ]\n",
    "    \n",
    "    return spark.createDataFrame(data, schema=get_opportunity_schema())\n",
    "\n",
    "def create_initial_delta_table(table_path):\n",
    "    \"\"\"\n",
    "    Create initial empty Delta table with schema\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating initial Delta table structure...\")\n",
    "    empty_df = spark.createDataFrame([], schema=get_opportunity_schema()) \\\n",
    "        .withColumn(\"inserted_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"updated_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "    \n",
    "    empty_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(table_path)\n",
    "    print(\"Initial table structure created\")\n",
    "\n",
    "def write_batch_to_source(df, source_path, batch_id):\n",
    "    \"\"\"\n",
    "    Write a batch of data to source directory\n",
    "    \"\"\"\n",
    "    batch_path = f\"{source_path}/batch_{batch_id}\"\n",
    "    df.write.format(\"json\").mode(\"overwrite\").save(batch_path)\n",
    "    print(f\"Written batch {batch_id} to {batch_path}\")\n",
    "\n",
    "def process_streaming_batch(df, epoch_id, target_table_path, changelog_path):\n",
    "    \"\"\"\n",
    "    Process each streaming batch with improved changelog\n",
    "    \"\"\"\n",
    "    batch_id = epoch_id + 1  # Adjust batch_id to start from 1\n",
    "    print(f\"\\nProcessing batch {batch_id}\")\n",
    "    \n",
    "    if df.isEmpty():\n",
    "        print(\"Empty batch, skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Read target Delta table\n",
    "    target_delta_table = DeltaTable.forPath(spark, target_table_path)\n",
    "    \n",
    "    # Get existing IDs\n",
    "    existing_ids = [row.OpportunityId for row in target_delta_table.toDF().select(\"OpportunityId\").collect()]\n",
    "    print(f\"Existing IDs in target: {existing_ids}\")\n",
    "    \n",
    "    # Create changelog entries\n",
    "    changelog_df = df.withColumn(\n",
    "        \"operation_type\",\n",
    "        when(col(\"OpportunityId\").isin(existing_ids), \"UPDATE\")\n",
    "        .otherwise(\"INSERT\")\n",
    "    ).withColumn(\"batch_id\", lit(batch_id)) \\\n",
    "      .withColumn(\"cdc_timestamp\", current_timestamp()) \\\n",
    "      .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Perform merge operation\n",
    "    merge_result = target_delta_table.alias(\"target\").merge(\n",
    "        source=df.alias(\"source\"),\n",
    "        condition=\"target.OpportunityId = source.OpportunityId\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"Amount\": \"source.Amount\",\n",
    "        \"StageName\": \"source.StageName\",\n",
    "        \"Probability\": \"source.Probability\",\n",
    "        \"LastModifiedDate\": \"source.LastModifiedDate\",\n",
    "        \"updated_timestamp\": \"current_timestamp()\"\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"OpportunityId\": \"source.OpportunityId\",\n",
    "        \"Name\": \"source.Name\",\n",
    "        \"Amount\": \"source.Amount\",\n",
    "        \"StageName\": \"source.StageName\",\n",
    "        \"CloseDate\": \"source.CloseDate\",\n",
    "        \"LastModifiedDate\": \"source.LastModifiedDate\",\n",
    "        \"AccountId\": \"source.AccountId\",\n",
    "        \"Probability\": \"source.Probability\",\n",
    "        \"inserted_timestamp\": \"current_timestamp()\",\n",
    "        \"updated_timestamp\": \"current_timestamp()\",\n",
    "        \"is_current\": \"true\"\n",
    "    }).execute()\n",
    "\n",
    "    # Write to changelog after successful merge\n",
    "    changelog_df.select(\n",
    "        \"OpportunityId\",\n",
    "        \"operation_type\",\n",
    "        \"Amount\",\n",
    "        \"StageName\",\n",
    "        \"Probability\",\n",
    "        \"LastModifiedDate\",\n",
    "        \"batch_id\",\n",
    "        \"processed_timestamp\"\n",
    "    ).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(changelog_path)\n",
    "    \n",
    "    print(f\"Completed processing batch {batch_id}\")\n",
    "\n",
    "def setup_streaming_query(source_path, target_table_path, changelog_path):\n",
    "    \"\"\"\n",
    "    Setup streaming query with explicit schema\n",
    "    \"\"\"\n",
    "    return spark.readStream \\\n",
    "        .format(\"cloudFiles\") \\\n",
    "        .option(\"cloudFiles.format\", \"json\") \\\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{source_path}/_schema\") \\\n",
    "        .schema(get_opportunity_schema()) \\\n",
    "        .load(source_path) \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(lambda df, epoch_id: process_streaming_batch(df, epoch_id, target_table_path, changelog_path)) \\\n",
    "        .option(\"checkpointLocation\", f\"{source_path}/_checkpoint\") \\\n",
    "        .trigger(once=True)\n",
    "\n",
    "def display_batch_results(target_table_path, changelog_path, batch_id):\n",
    "    \"\"\"\n",
    "    Display results after each batch with improved formatting\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Results After Batch {batch_id}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Main table status\n",
    "    print(\"\\nMain Table Contents:\")\n",
    "    main_df = spark.read.format(\"delta\").load(target_table_path)\n",
    "    main_df.orderBy(\"OpportunityId\").select(\n",
    "        \"OpportunityId\", \n",
    "        \"Name\", \n",
    "        \"Amount\", \n",
    "        \"StageName\", \n",
    "        \"Probability\",\n",
    "        \"LastModifiedDate\"\n",
    "    ).show()\n",
    "    \n",
    "    # Changelog for this batch\n",
    "    print(f\"\\nChanges in Batch {batch_id}:\")\n",
    "    changelog_df = spark.read.format(\"delta\").load(changelog_path)\n",
    "    batch_changes = changelog_df.filter(col(\"batch_id\") == batch_id) \\\n",
    "        .orderBy(\"OpportunityId\") \\\n",
    "        .select(\n",
    "            \"OpportunityId\",\n",
    "            \"operation_type\",\n",
    "            \"Amount\",\n",
    "            \"StageName\",\n",
    "            \"Probability\",\n",
    "            \"LastModifiedDate\"\n",
    "        )\n",
    "    batch_changes.show()\n",
    "\n",
    "    # Show operation counts for this batch\n",
    "    print(f\"\\nOperation Summary for Batch {batch_id}:\")\n",
    "    batch_changes.groupBy(\"operation_type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"operation_type\") \\\n",
    "        .show()\n",
    "\n",
    "def display_final_summary(changelog_path):\n",
    "    \"\"\"\n",
    "    Display final summary of all changes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Change Summary\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    changelog_df = spark.read.format(\"delta\").load(changelog_path)\n",
    "    \n",
    "    print(\"\\nAll Changes by Batch:\")\n",
    "    changelog_df.groupBy(\"batch_id\", \"operation_type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"batch_id\", \"operation_type\") \\\n",
    "        .show()\n",
    "    \n",
    "    print(\"\\nTotal Changes by Operation Type:\")\n",
    "    changelog_df.groupBy(\"operation_type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"operation_type\") \\\n",
    "        .show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution flow\n",
    "    \"\"\"\n",
    "    source_path = \"/tmp/opportunity_source\"\n",
    "    target_table_path = \"/tmp/opportunity_delta\"\n",
    "    changelog_path = \"/tmp/opportunity_changelog\"\n",
    "    \n",
    "    try:\n",
    "        # Clean up and initialize\n",
    "        cleanup_tables(source_path, target_table_path, changelog_path)\n",
    "        create_initial_delta_table(target_table_path)\n",
    "        \n",
    "        # Process each batch\n",
    "        for batch_id in range(1, 4):\n",
    "            print(f\"\\nProcessing Batch {batch_id}\")\n",
    "            \n",
    "            # Generate and write batch data\n",
    "            batch_df = generate_sample_data(batch_id)\n",
    "            write_batch_to_source(batch_df, source_path, batch_id)\n",
    "            \n",
    "            # Process the batch\n",
    "            query = setup_streaming_query(source_path, target_table_path, changelog_path).start()\n",
    "            query.awaitTermination()\n",
    "            \n",
    "            # Display results after each batch\n",
    "            display_batch_results(target_table_path, changelog_path, batch_id)\n",
    "        \n",
    "        # Show final summary\n",
    "        display_final_summary(changelog_path)\n",
    "        print(\"\\nStreaming pipeline completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in streaming pipeline: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427818b0-209c-4111-ad8b-d95b8896cba9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Batch"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCleaning up existing tables...\n\nSTEP 1: Performing initial load...\n\nSTEP 2: Processing CDC updates...\n\n================================================================================\nCDC PIPELINE EXECUTION RESULTS\n================================================================================\n\n1. INITIAL DATA IN TABLE\n--------------------------------------------------------------------------------\n+-------------+------+-------+-------------+-----------+--------------------+\n|OpportunityId|  Name| Amount|    StageName|Probability|  inserted_timestamp|\n+-------------+------+-------+-------------+-----------+--------------------+\n|       OPP001|Deal A|12000.0|Qualification|       30.0|2025-05-05 03:37:...|\n|       OPP002|Deal B|25000.0|  Negotiation|       60.0|2025-05-05 03:37:...|\n|       OPP003|Deal C|15000.0|   Closed Won|      100.0|2025-05-05 03:37:...|\n|       OPP004|Deal D|30000.0|  Prospecting|       20.0|2025-05-05 03:38:...|\n+-------------+------+-------+-------------+-----------+--------------------+\n\n\n2. CHANGES PROCESSED\n--------------------------------------------------------------------------------\n+-------------+--------------+-------+-------------+-----------+--------------------+\n|OpportunityId|operation_type| Amount|    StageName|Probability| processed_timestamp|\n+-------------+--------------+-------+-------------+-----------+--------------------+\n|       OPP001|        UPDATE|12000.0|Qualification|       30.0|2025-05-05 03:37:...|\n|       OPP002|        UPDATE|25000.0|  Negotiation|       60.0|2025-05-05 03:37:...|\n|       OPP003|        UPDATE|15000.0|   Closed Won|      100.0|2025-05-05 03:37:...|\n|       OPP004|        INSERT|30000.0|  Prospecting|       20.0|2025-05-05 03:37:...|\n+-------------+--------------+-------+-------------+-----------+--------------------+\n\n\n3. FINAL DATA AFTER CHANGES\n--------------------------------------------------------------------------------\n+-------------+------+-------+-------------+-----------+--------------------+\n|OpportunityId|  Name| Amount|    StageName|Probability|   updated_timestamp|\n+-------------+------+-------+-------------+-----------+--------------------+\n|       OPP001|Deal A|12000.0|Qualification|       30.0|2025-05-05 03:38:...|\n|       OPP002|Deal B|25000.0|  Negotiation|       60.0|2025-05-05 03:38:...|\n|       OPP003|Deal C|15000.0|   Closed Won|      100.0|2025-05-05 03:38:...|\n|       OPP004|Deal D|30000.0|  Prospecting|       20.0|2025-05-05 03:38:...|\n+-------------+------+-------+-------------+-----------+--------------------+\n\n\n4. CHANGE SUMMARY\n--------------------------------------------------------------------------------\n\nOperations Performed:\n+--------------+-----+\n|operation_type|count|\n+--------------+-----+\n|        UPDATE|    3|\n|        INSERT|    1|\n+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure Delta table settings\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "def cleanup_tables(table_path, changelog_path):\n",
    "    \"\"\"\n",
    "    Clean up existing tables before running the pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning up existing tables...\")\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS delta.`{table_path}`\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS delta.`{changelog_path}`\")\n",
    "        dbutils.fs.rm(table_path, recurse=True)\n",
    "        dbutils.fs.rm(changelog_path, recurse=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup warning (safe to ignore if first run): {str(e)}\")\n",
    "\n",
    "def generate_sample_data(is_update=False):\n",
    "    \"\"\"\n",
    "    Generate mock Salesforce Opportunity data\n",
    "    \"\"\"\n",
    "    if not is_update:\n",
    "        # Initial data\n",
    "        data = [\n",
    "            (\"OPP001\", \"Deal A\", 10000.0, \"Prospecting\", \"2023-01-01\", \"2023-01-01 10:00:00\", \"ACC001\", 20.0),\n",
    "            (\"OPP002\", \"Deal B\", 25000.0, \"Negotiation\", \"2023-02-01\", \"2023-01-02 11:00:00\", \"ACC002\", 60.0),\n",
    "            (\"OPP003\", \"Deal C\", 15000.0, \"Proposal\", \"2023-03-01\", \"2023-01-03 12:00:00\", \"ACC003\", 40.0)\n",
    "        ]\n",
    "    else:\n",
    "        # Modified data for testing updates\n",
    "        data = [\n",
    "            (\"OPP001\", \"Deal A\", 12000.0, \"Qualification\", \"2023-01-01\", \"2023-01-04 10:00:00\", \"ACC001\", 30.0),\n",
    "            (\"OPP002\", \"Deal B\", 25000.0, \"Negotiation\", \"2023-02-01\", \"2023-01-02 11:00:00\", \"ACC002\", 60.0),\n",
    "            (\"OPP003\", \"Deal C\", 15000.0, \"Closed Won\", \"2023-03-01\", \"2023-01-05 12:00:00\", \"ACC003\", 100.0),\n",
    "            (\"OPP004\", \"Deal D\", 30000.0, \"Prospecting\", \"2023-04-01\", \"2023-01-05 14:00:00\", \"ACC004\", 20.0)\n",
    "        ]\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"OpportunityId\", StringType(), False),\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"Amount\", DoubleType(), True),\n",
    "        StructField(\"StageName\", StringType(), True),\n",
    "        StructField(\"CloseDate\", StringType(), True),\n",
    "        StructField(\"LastModifiedDate\", StringType(), True),\n",
    "        StructField(\"AccountId\", StringType(), True),\n",
    "        StructField(\"Probability\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "def create_initial_delta_table(df, table_path):\n",
    "    \"\"\"\n",
    "    Create initial Delta table with the source data\n",
    "    \"\"\"\n",
    "    df_with_metadata = df.withColumn(\"inserted_timestamp\", current_timestamp()) \\\n",
    "                        .withColumn(\"updated_timestamp\", current_timestamp()) \\\n",
    "                        .withColumn(\"is_current\", lit(True))\n",
    "    \n",
    "    df_with_metadata.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(table_path)\n",
    "\n",
    "def process_cdc_updates(source_df, target_table_path, changelog_path):\n",
    "    \"\"\"\n",
    "    Process CDC updates and maintain changelog\n",
    "    \"\"\"\n",
    "    # Read existing Delta table\n",
    "    target_delta_table = DeltaTable.forPath(spark, target_table_path)\n",
    "    \n",
    "    # Get existing IDs before merge\n",
    "    existing_ids = [row.OpportunityId for row in target_delta_table.toDF().select(\"OpportunityId\").collect()]\n",
    "    \n",
    "    # Prepare source data with metadata\n",
    "    source_with_metadata = source_df.withColumn(\"cdc_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Create simple changelog entries\n",
    "    changelog_df = source_with_metadata.withColumn(\n",
    "        \"operation_type\",\n",
    "        when(col(\"OpportunityId\").isin(existing_ids), \"UPDATE\")\n",
    "        .otherwise(\"INSERT\")\n",
    "    ).withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Write to changelog\n",
    "    changelog_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(changelog_path)\n",
    "    \n",
    "    # Perform merge operation\n",
    "    merge_result = target_delta_table.alias(\"target\").merge(\n",
    "        source=source_with_metadata.alias(\"source\"),\n",
    "        condition=\"target.OpportunityId = source.OpportunityId\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"Amount\": \"source.Amount\",\n",
    "        \"StageName\": \"source.StageName\",\n",
    "        \"Probability\": \"source.Probability\",\n",
    "        \"LastModifiedDate\": \"source.LastModifiedDate\",\n",
    "        \"updated_timestamp\": \"current_timestamp()\"\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"OpportunityId\": \"source.OpportunityId\",\n",
    "        \"Name\": \"source.Name\",\n",
    "        \"Amount\": \"source.Amount\",\n",
    "        \"StageName\": \"source.StageName\",\n",
    "        \"CloseDate\": \"source.CloseDate\",\n",
    "        \"LastModifiedDate\": \"source.LastModifiedDate\",\n",
    "        \"AccountId\": \"source.AccountId\",\n",
    "        \"Probability\": \"source.Probability\",\n",
    "        \"inserted_timestamp\": \"current_timestamp()\",\n",
    "        \"updated_timestamp\": \"current_timestamp()\",\n",
    "        \"is_current\": \"true\"\n",
    "    }).execute()\n",
    "\n",
    "def display_pipeline_results(table_path, changelog_path):\n",
    "    \"\"\"\n",
    "    Display CDC pipeline results in a clear, structured format\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CDC PIPELINE EXECUTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Initial Data\n",
    "    print(\"\\n1. INITIAL DATA IN TABLE\")\n",
    "    print(\"-\"*80)\n",
    "    initial_df = spark.read.format(\"delta\").load(table_path).orderBy(\"OpportunityId\")\n",
    "    initial_df.select(\n",
    "        \"OpportunityId\", \"Name\", \"Amount\", \"StageName\", \"Probability\", \n",
    "        \"inserted_timestamp\"\n",
    "    ).show()\n",
    "\n",
    "    # 2. Changes Processed\n",
    "    print(\"\\n2. CHANGES PROCESSED\")\n",
    "    print(\"-\"*80)\n",
    "    changelog_df = spark.read.format(\"delta\").load(changelog_path)\n",
    "    changelog_df.select(\n",
    "        \"OpportunityId\",\n",
    "        \"operation_type\",\n",
    "        \"Amount\",\n",
    "        \"StageName\",\n",
    "        \"Probability\",\n",
    "        \"processed_timestamp\"\n",
    "    ).orderBy(\"OpportunityId\").show()\n",
    "\n",
    "    # 3. Final Data\n",
    "    print(\"\\n3. FINAL DATA AFTER CHANGES\")\n",
    "    print(\"-\"*80)\n",
    "    final_df = spark.read.format(\"delta\").load(table_path).orderBy(\"OpportunityId\")\n",
    "    final_df.select(\n",
    "        \"OpportunityId\", \"Name\", \"Amount\", \"StageName\", \"Probability\", \n",
    "        \"updated_timestamp\"\n",
    "    ).show()\n",
    "\n",
    "    # 4. Change Summary\n",
    "    print(\"\\n4. CHANGE SUMMARY\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"\\nOperations Performed:\")\n",
    "    changelog_df.groupBy(\"operation_type\").count().show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution flow\n",
    "    \"\"\"\n",
    "    table_path = \"/tmp/opportunity_delta\"\n",
    "    changelog_path = \"/tmp/opportunity_changelog\"\n",
    "    \n",
    "    try:\n",
    "        # Clean up existing tables\n",
    "        cleanup_tables(table_path, changelog_path)\n",
    "        \n",
    "        # Step 1: Initial Load\n",
    "        print(\"\\nSTEP 1: Performing initial load...\")\n",
    "        initial_df = generate_sample_data()\n",
    "        create_initial_delta_table(initial_df, table_path)\n",
    "        \n",
    "        # Step 2: Process Updates\n",
    "        print(\"\\nSTEP 2: Processing CDC updates...\")\n",
    "        updated_df = generate_sample_data(is_update=True)\n",
    "        process_cdc_updates(updated_df, table_path, changelog_path)\n",
    "        \n",
    "        # Step 3: Display Results\n",
    "        display_pipeline_results(table_path, changelog_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in pipeline execution: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "226921f3-0d69-40e1-b4da-9f921ebc2a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If the unity catalog configured with the hive metastore , we can directly use the schemas , such it gets pointed the storaage account \n",
    "\n",
    "-- Create and configure the complete hierarchy with necessary permissions\n",
    "CREATE CATALOG IF NOT EXISTS salesforce_cdc;\n",
    "USE CATALOG salesforce_cdc;\n",
    "\n",
    "-- Create schemas\n",
    "CREATE SCHEMA IF NOT EXISTS main;\n",
    "CREATE SCHEMA IF NOT EXISTS audit;\n",
    "\n",
    "-- Register main opportunity table with lineage and governance\n",
    "CREATE TABLE main.opportunity\n",
    "LOCATION '/tmp/opportunity_delta'\n",
    "COMMENT 'Salesforce Opportunity CDC data'\n",
    "TBLPROPERTIES (\n",
    "    'delta.enableChangeDataFeed' = 'true',\n",
    "    'delta.columnMapping.mode' = 'name',\n",
    "    'pipeline.owner' = 'data_engineering',\n",
    "    'data.source' = 'salesforce',\n",
    "    'data.sensitivity' = 'confidential',\n",
    "    'lineage.upstream' = 'salesforce.opportunity',\n",
    "    'audit.enabled' = 'true'\n",
    ");\n",
    "\n",
    "-- Register changelog table\n",
    "CREATE TABLE audit.opportunity_changelog\n",
    "LOCATION '/tmp/opportunity_changelog'\n",
    "COMMENT 'Change tracking log'\n",
    "TBLPROPERTIES (\n",
    "    'delta.enableChangeDataFeed' = 'true',\n",
    "    'audit.retention.days' = '90'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56466184-117e-4af1-b535-9f5670aeadb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming Ingestion and CDC Tracking in Databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}